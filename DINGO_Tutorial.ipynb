{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PrqX-YAHh_KB"
   },
   "source": [
    "# DINGO Tutorial\n",
    "\n",
    "**Welcome to the DINGO tutorial!**\n",
    "\n",
    "This notebook provides an introduction to the `dingo` machine-learning package for gravitational wave parameter estimation. It shows how to train a DINGO model from scratch or use an already-trained model to obtain posterior samples for gravitational wave events. We hope that this will serve as a starting point for those interested in using DINGO.\n",
    "\n",
    "For a more general introduction to neural posterior estimation of gravitational waves (without DINGO), see the tutorial [\"GW Parameter Inference with Machine Learning\"](https://github.com/stephengreen/gw-school-corfu-2023).\n",
    "\n",
    "Since it takes several days to train a production-level DINGO model [14 parameters, O(100 million) network parameters, O(10 million) training examples], we make several simplifications:\n",
    "- Restrict the parameter space to two parameters (chirp mass and the mass ratio).\n",
    "- Use a much smaller network and training dataset.\n",
    "This will give reduced performance compared to the full model, but can be run in real time.\n",
    "\n",
    "## Tutorial Structure\n",
    "1. Generating a waveform dataset\n",
    "2. Generating an ASD dataset\n",
    "3. Training\n",
    "4. Inference on injections\n",
    "5. Inference on a real event with `dingo_pipe`\n",
    "6. Inference with a pre-trained DINGO model (from Zenodo)\n",
    "\n",
    "If you want to run on this notebook on GPU, make sure that `Runtime` -> `Change runtime type` -> GPU.\n",
    "If you want to run on CPU, you need to change `device = cuda` to `device = cpu` in the settings used for training and inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I0Wnnk1rlRFe"
   },
   "source": [
    "## 0. Installation\n",
    "\n",
    "We use `pip`, but dingo can also be installed using `conda`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I88mekKClTda",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip3 install dingo-gw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LP3pxcCwK42O"
   },
   "source": [
    "To check whether `dingo` is correctly installed and ready to use, run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "imgDaLuRK_pP",
    "outputId": "85e62a21-fa94-4406-8f12-74685ff7f980"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", \"Wswiglal-redir-stdio\")\n",
    "import lal\n",
    "\n",
    "import dingo\n",
    "print(dingo.__version__) # Should be >= 0.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7gALmOI8V5Nt"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3_Ft58QApNXR"
   },
   "source": [
    "Prepare folder structure for tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D0C_WqEfpN2A"
   },
   "outputs": [],
   "source": [
    "# Remove automatically created folder 'sample_data'\n",
    "os.system(\"rm -rf sample_data\")\n",
    "# Create folders required for the tutorial\n",
    "os.makedirs('01_training_data/asd_dataset', exist_ok=True)\n",
    "os.makedirs('01_training_data/waveform_dataset', exist_ok=True)\n",
    "os.makedirs('02_training', exist_ok=True)\n",
    "os.makedirs('03_inference/injection', exist_ok=True)\n",
    "os.makedirs('04_exercise/lum_dist_marginalization', exist_ok=True)\n",
    "os.makedirs('04_exercise/with_lum_dist', exist_ok=True)\n",
    "os.makedirs('05_pretrained_model/init_train_dir', exist_ok=True)\n",
    "os.makedirs('05_pretrained_model/main_train_dir', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XzYmfqu1lNH2"
   },
   "source": [
    "## 1. Generating a wavefrom dataset\n",
    "**Step 1: Prepare waveform dataset settings file**\n",
    "\n",
    "DINGO works with .yaml settings files that set all important parameters for generating data and training a model. In the following, we prepare a settings file to generate a waveform dataset on which the DINGO model will be trained later.\n",
    "\n",
    "A waveform dataset settings file contains the following information:\n",
    "- **domain**: Specifies frequency domain within the range $[f_\\mathrm{min}, f_\\mathrm{max}]$ and $\\delta f$. Time domain is currently not supported.\n",
    "- **waveform_generator**: Specify waveform approximant (e.g., IMRPhenomPv2, IMRPhenomXPHM, SEOBNRv4PHM, SEOBNRv5PHM) and reference frequency. Note that SEOBNRv5 waveforms require `pyseobnr` to be installed separately (optional).\n",
    "- **intrinsic_prior**: The parameter space is split into intrinsic and extrinsic components: *Intrinsic* parameters refer to those that are needed to generate waveform polarizations. *Extrinsic* parameters refer to those parameters that can be sampled and applied rapidly during training. Luminosity distance and time of coalescence are considered as both intrinsic and extrinsic. They are needed to generate polarizations, but they can also be easily transformed during training to augment the dataset. We therefore fix them to fiducial values for generating polarizations.\n",
    "- **num_samples**: Number of waveforms to generate. In a production setting, it is recommended to train a model with (at least) 5,000,000 waveforms, with larger parameter space requiring more samples.\n",
    "- **compression**: It is recommended to save compressed waveforms using an SVD basis.\n",
    "\n",
    "For the tutorial, we analyze only chirp mass and the mass ratio, fixing all other parameters\n",
    "\n",
    "More information about generating the waveform dataset can be found in the sections [Generating Waveforms](https://dingo-gw.readthedocs.io/en/latest/generating_waveforms.html) and [Building a waveform dataset](https://dingo-gw.readthedocs.io/en/latest/waveform_dataset.html) of the DINGO documentation.\n",
    "\n",
    "For more details on how to parallelize this step over multiple cores on a `htcondor` cluster, you can find more information in one of the [production tutorials](https://dingo-gw.readthedocs.io/en/latest/example_npe_model.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Skxp2SKkqDP6"
   },
   "outputs": [],
   "source": [
    "waveform_dataset_settings = \"\"\"\n",
    "domain:\n",
    "  type: FrequencyDomain\n",
    "  f_min: 20.0\n",
    "  f_max: 256.0\n",
    "  delta_f: 0.5 # Expressions like 1.0/8.0 would require eval and are not supported\n",
    "\n",
    "waveform_generator:\n",
    "  approximant: IMRPhenomPv2\n",
    "  f_ref: 20.0\n",
    "  spin_conversion_phase: 0.0   # Reference phase when converting from spin angles to Cartesian spins. If None, use phase parameter.\n",
    "  # f_start: 15.0  # Optional setting useful for EOB waveforms. Overrides f_min when generating waveforms.\n",
    "\n",
    "# Dataset only samples over intrinsic parameters. Extrinsic parameters are chosen at train time.\n",
    "intrinsic_prior:\n",
    "  mass_1: bilby.core.prior.Constraint(minimum=20.0, maximum=40.0)\n",
    "  mass_2: bilby.core.prior.Constraint(minimum=20.0, maximum=40.0)\n",
    "  chirp_mass: bilby.gw.prior.UniformInComponentsChirpMass(minimum=15.0, maximum=50.0)\n",
    "  mass_ratio: bilby.gw.prior.UniformInComponentsMassRatio(minimum=0.125, maximum=1.0)\n",
    "  theta_jn: 2.624497413635254\n",
    "  tilt_1: 2.0111560821533203\n",
    "  tilt_2: 1.0743615627288818\n",
    "  a_1: 0.925635814666748\n",
    "  a_2: 0.5538952350616455\n",
    "  phi_jl: 5.561878204345703\n",
    "  phase: 0.9604566579018894\n",
    "  # Reference values for fixed (extrinsic) parameters. These are needed to generate a waveform.\n",
    "  luminosity_distance: 100.0  # Mpc\n",
    "  geocent_time: 0.0 # s\n",
    "\n",
    "# Dataset size\n",
    "num_samples: 10_000\n",
    "\n",
    "compression: None\n",
    "\"\"\"\n",
    "waveform_dataset_settings = yaml.safe_load(waveform_dataset_settings)\n",
    "with open('01_training_data/waveform_dataset/waveform_dataset_settings.yaml', 'w') as outfile:\n",
    "    yaml.dump(waveform_dataset_settings, outfile, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fuq9gUhWqDdd"
   },
   "source": [
    "\n",
    "**Step 2: Generate waveform dataset**\n",
    "\n",
    "To generate the waveform dataset, execute the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TIS56_UYlQUE",
    "outputId": "fba1875b-40a2-4c00-ce49-7dd93d115543"
   },
   "outputs": [],
   "source": [
    "!dingo_generate_dataset  \\\n",
    "--settings 01_training_data/waveform_dataset/waveform_dataset_settings.yaml \\\n",
    "--out_file 01_training_data/waveform_dataset/waveform_dataset.hdf5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3eCI_XVAbyy4"
   },
   "source": [
    "Load the dataset and visualize an exemplary waveform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M0cnwWp4RA_4",
    "outputId": "4d1991c5-66a1-4fe5-a236-b1dc666a619b"
   },
   "outputs": [],
   "source": [
    "from dingo.gw.dataset.waveform_dataset import WaveformDataset\n",
    "\n",
    "# Load dataset\n",
    "waveform_dataset_path = '01_training_data/waveform_dataset/waveform_dataset.hdf5'\n",
    "wfd = WaveformDataset(file_name=waveform_dataset_path)\n",
    "print(\"One Datapoint contains:\", wfd[0].keys())\n",
    "print(\"Parameters contains: \", wfd[0]['parameters'].keys())\n",
    "print(\"Waveform contains:\", wfd[0]['waveform'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 469
    },
    "id": "N5vRbXV6Ud76",
    "outputId": "65e4b343-fc32-40ad-9733-82b294aef70a"
   },
   "outputs": [],
   "source": [
    "# Plot an exemplary waveform\n",
    "f_domain = wfd.domain.sample_frequencies\n",
    "data_sample = wfd[0]['waveform']\n",
    "\n",
    "plt.plot(f_domain, data_sample['h_cross'].real, c=\"blue\", label=r'$h_{\\times}$ real')\n",
    "plt.plot(f_domain, data_sample['h_cross'].imag, c=\"cornflowerblue\", label=r'$h_{\\times}$ imag')\n",
    "plt.plot(f_domain, data_sample['h_plus'].real, c=\"tab:red\", label=r'$h_+$ real')\n",
    "plt.plot(f_domain, data_sample['h_plus'].imag, c=\"coral\", label=r'$h_+$ imag')\n",
    "plt.xlabel(r\"Frequency $f$ [Hz]\")\n",
    "plt.ylabel(r\"Polarization $h$\")\n",
    "plt.legend();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6lxrXzG7aY9G"
   },
   "source": [
    "As an exmple for what you can do with DINGO, we apply a time-shift to the data point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 368
    },
    "id": "qD9-M-IAYaKD",
    "outputId": "731c478b-3048-46f9-c8fb-91c61b30d6b8"
   },
   "outputs": [],
   "source": [
    "# Convert dictionary into numpy array\n",
    "data_sample_np = np.array([data_sample['h_cross'], data_sample['h_plus']])\n",
    "\n",
    "# Time-translate sample\n",
    "time_translated_sample = wfd.domain.time_translate_data(data_sample_np, dt=0.1)\n",
    "\n",
    "# Plot\n",
    "fig, axs = plt.subplots(1, 2, sharey=True, figsize=(8,3.5))\n",
    "axs[0].plot(f_domain, data_sample['h_cross'].real, c='blue', label=r'$h_{\\times}$ real')\n",
    "axs[0].plot(f_domain, data_sample['h_plus'].real, c='tab:red', label=r'$h_+$ real')\n",
    "axs[0].set_xlabel(r\"Frequency $f$ [Hz]\")\n",
    "axs[0].set_ylabel(r\"Polarization $h$\")\n",
    "axs[0].legend()\n",
    "\n",
    "axs[1].plot(f_domain, time_translated_sample[0].real, c=\"blue\", label=r'Time-translated $h_{\\times}$ real')\n",
    "axs[1].plot(f_domain, time_translated_sample[1].real, c=\"tab:red\", label=r'Time-translated $h_+$ real')\n",
    "axs[1].set_xlabel(r\"Frequency $f$ [Hz]\")\n",
    "axs[1].legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_tqiDrAHlUZ3"
   },
   "source": [
    "## 2. Generating an ASD dataset\n",
    "**Step 1: Prepare settings file**\n",
    "\n",
    "To generate the ASD dataset, we need to prepare a settings file which contains the following information within `dataset_settings`:\n",
    "- **f_min, f_max** (Optional)\n",
    "- **f_s**: Sampling rate. This should be at least twice the value of `f_max` expected to be used.\n",
    "- **time_psd**: The entire length of data from which to estimate a PSD using Welch's method. Periodigrams are calculated on segments of this, and then averaged using the median method.\n",
    "- **T**: The length of each segment on which to take the DFT and calculate a periodigram.\n",
    "- **window**: Parameters of the window function used before taking DFT of data segments.\n",
    "- **channels** (Optional): Channels where to download the data from. By default will use open data from GWOSC\n",
    "- **time_gap**: This sets the time that is skipped between consecutive PSD estimates. If set < 0., the time segments overlap.\n",
    "- **num_psds_max**: If this is set to 0, all available PSDs will be downloaded. For values > 0, a subset of all available PSDs is used.\n",
    "- **detectors**: This list specifies for which detectors we want to download ASDs. Options: H1, L1, V1.\n",
    "- **observing_run**: This sets the observing run of the ASDs. Options: O1, O2, O3.\n",
    "\n",
    "In a DINGO training run where the initial layers of the embedding network are seeded with SVD components, it is necessary to generate two ASD datasets: One with a single, fiducial ASD that is fixed during the first training stage, and a second one with all ASDs of the selected observing run. In this tutorial, we will train the network with a single ASD.\n",
    "\n",
    "More information about the noise modeling with DINGO can be found in the section [Detector Noise](https://dingo-gw.readthedocs.io/en/latest/noise_dataset.html) of the documentation. It also includes examples for `htcondor` settings that allow to run this step on the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_3bcFGmiq1Sv"
   },
   "outputs": [],
   "source": [
    "asd_dataset_settings = \"\"\"\n",
    "dataset_settings:\n",
    "#  f_min: 0         # defaults to 0\n",
    "#  f_max: 2048      # defaults to f_s/2\n",
    "  f_s: 4096\n",
    "  time_psd: 1024\n",
    "  T: 2.0\n",
    "  window:\n",
    "    roll_off: 0.4\n",
    "    type: tukey\n",
    "  time_gap: 0 # specifies the time skipped between to consecutive PSD estimates. If set < 0, the time segments overlap\n",
    "  num_psds_max: 1 # if set > 0, only a subset of all available PSDs will be used\n",
    "  detectors:\n",
    "    - H1\n",
    "    - L1\n",
    "  observing_run: O1\n",
    "\"\"\"\n",
    "asd_dataset_settings = yaml.safe_load(asd_dataset_settings)\n",
    "with open('01_training_data/asd_dataset/asd_dataset_settings.yaml', 'w') as outfile:\n",
    "    yaml.dump(asd_dataset_settings, outfile, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IGHv3EGTq2Ag"
   },
   "source": [
    "**Step 2: Generate ASD dataset**\n",
    "\n",
    "If we execute the command `dingo_generate_asd_dataset` with the following arguments, it would automatically download the first valid ASD time segment of the selected observing run. (For O1, this is 1126073529 - 1126074553.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MO5gB3Iz7z8H",
    "outputId": "0876dbf0-c4e9-41c9-a624-fd196c4a3fc5"
   },
   "outputs": [],
   "source": [
    "!dingo_generate_asd_dataset \\\n",
    "--settings_file 01_training_data/asd_dataset/asd_dataset_settings.yaml \\\n",
    "--data_dir 01_training_data/asd_dataset --out_name 01_training_data/asd_dataset/asd_fiducial_O1.hdf5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FFppPrhn74xd"
   },
   "source": [
    "However, we are interested in analyzing GW150914. Therefore, we specify the start and end time of this event in `time_segment.pkl` and provide it as an additional argument to `dingo_generate_asd_dataset`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Av8XjoV6uzV_"
   },
   "outputs": [],
   "source": [
    "time_GW150914 = 1126259462.391 - 0.0114 # maxlog geocent time of GW150914\n",
    "asd_start_time = time_GW150914 - asd_dataset_settings[\"dataset_settings\"][\"T\"]\n",
    "asd_end_time = time_GW150914\n",
    "time_segments = {\n",
    "    'H1': [[asd_start_time, asd_end_time]],\n",
    "    'L1': [[asd_start_time, asd_end_time]]\n",
    "}\n",
    "\n",
    "with open('01_training_data/asd_dataset/time_segment_GW150914.pkl', 'wb') as f:\n",
    "    pickle.dump(time_segments, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hGK6VNrazQnp",
    "outputId": "b094ec31-037b-4d79-eb2f-a32a8dc7847b"
   },
   "outputs": [],
   "source": [
    "!dingo_generate_asd_dataset \\\n",
    "--settings_file 01_training_data/asd_dataset/asd_dataset_settings.yaml \\\n",
    "--data_dir 01_training_data/asd_dataset --out_name 01_training_data/asd_dataset/asd_GW150914.hdf5 \\\n",
    "--time_segments_file 01_training_data/asd_dataset/time_segment_GW150914.pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mE8qD8RNcwVA"
   },
   "source": [
    "Load and visualize exemplary ASD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 496
    },
    "id": "SSJizLYSlhtO",
    "outputId": "39de6c73-2276-4410-8bf8-afbebc286951"
   },
   "outputs": [],
   "source": [
    "from dingo.gw.noise.asd_dataset import ASDDataset\n",
    "\n",
    "# Load ASD dataset\n",
    "asd_dataset_path = '01_training_data/asd_dataset/asd_GW150914.hdf5'\n",
    "asds = ASDDataset(file_name=asd_dataset_path, domain_update=wfd.domain.domain_dict)\n",
    "# Get ASD sample\n",
    "asd_sample = asds.sample_random_asds()\n",
    "print(\"One Datapoint contains:\", asd_sample.keys())\n",
    "\n",
    "plt.loglog(f_domain, asd_sample['H1'], label=r'H1')\n",
    "plt.loglog(f_domain, asd_sample['L1'], label=r'L1')\n",
    "plt.xlabel(r\"Frequency [Hz]\")\n",
    "plt.ylabel(r\"Noise ASD $[1/\\sqrt{\\mathrm{Hz}}]$\")\n",
    "plt.xlim([wfd.domain.f_min, wfd.domain.f_max])\n",
    "plt.ylim([1.e-24, 1.e-20])\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R4xKhg7TlZ1B"
   },
   "source": [
    "## 3. Training a simple model\n",
    "**Step 1: Prepare training config file**\n",
    "\n",
    "The train settings file contains multiple categories:\n",
    "\n",
    "**(a) data**: Information about training data.\n",
    "- **waveform_dataset_path**: Path to the waveform dataset which contains the intrinsic waveforms\n",
    "- **train_fraction**: Fraction of waveform dataset to be used for training. The remainder are used to compute the test loss.\n",
    "- **window**: Defines the window function to use when FFTing the time-domain data. It is used here to calculate a window factor for simulating data. See discussion [here](https://dingo-gw.readthedocs.io/en/latest/noise_dataset.html#ref-window-factor).\n",
    "- **detectors**: List of detectors that should be used during training. Options: H1, L1, V1. Note: Dingo models are trained for a *fixed set of detectors*. This must be selected prior to training, and a new model must be trained if one wishes to analyze data in a different set of detectors. Thus, e.g., separate models must be trained for HL and HLV configurations.\n",
    "- **extrinsic_prior**: Specify the extrinsic prior. Default options are available.\n",
    "- **ref_time**: Reference time for the interferometer locations and orientations. See the important note [here](https://dingo-gw.readthedocs.io/en/latest/training_transforms.html#ref-ref-time).\n",
    "- **inference_parameters**: Parameters to infer with the model. At present they must be a subset of `sample[\"parameters\"]`. By specifying a strict subset, this can be used to marginalize over parameters. The default setting points to `dingo.gw.prior.default_inference_parameters`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6bMHJfdNDH5f",
    "outputId": "63f9b0f4-b86a-4e4d-ab50-1eee1e66e278"
   },
   "outputs": [],
   "source": [
    "from dingo.gw.prior import default_inference_parameters\n",
    "default_inference_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nHGVQBonECLz"
   },
   "source": [
    "\n",
    "**(b) model**: Details about the DINGO model.\n",
    "- **type**: Type of model, usually: `nsf+embedding`\n",
    "- **nsf_kwargs**: Kwargs of the `nsf` such as number of subsequent flow steps, kwargs of flow transform `base_transform_kwargs`.\n",
    "- **embedding_net_kwargs**: Kwargs of the embedding network that compresses the data to a vector which is provided as a condition to the normalizing flow. E.g. dimension of output vector `out_dim`, hidden dimensions `hidden_dims`, how to obtain the SVD components for the initialization of the first layer `svd`.\n",
    "\n",
    "**(c) training**: Settings for the training procedure, divided into stages. Each stage contains:\n",
    "- **epochs**: Number of epochs\n",
    "- **asd_dataset_path**: Path to the ASD dataset. If the DINGO model is trained in two stages, this should just contain a single fiducial ASD per detector for the first stage and a full ASD dataset per detector for the second stage.\n",
    "- **freeze_rb_layer**: Whether to freeze the reduced basis layer in the embedding network in this stage.\n",
    "- **optimizer**: Optimizer setings.\n",
    "- **scheduler**: Scheduler settings.\n",
    "- **batch_size**: Number of data points to use per batch.\n",
    "\n",
    "\n",
    "**(d) local**: Technical settings (no influence on final model).\n",
    "- **device**: Which device to train on. Options: `cpu`, `cuda` for training on GPU.\n",
    "- **num_workers**: Number of workers that are used to preprocess the data before training. (`num_workers >0` does not work on Mac, see [post](https://stackoverflow.com/questions/64772335/pytorch-w-parallelnative-cpp206))\n",
    "- **runtime_limits**: Specifies the maximum time per run and the maximum number of epochs per run.\n",
    "- **checkpoint_epochs**: Sets after how many epochs a checkpoint is saved.\n",
    "\n",
    "For more information about which default values should be used for production, please refer to the tutorials for [training an NPE model](https://dingo-gw.readthedocs.io/en/latest/example_npe_model.html), [training a GNPE model](https://dingo-gw.readthedocs.io/en/latest/example_gnpe_model.html). Furthermore, the documentation contains more details about the [network architecture](https://dingo-gw.readthedocs.io/en/latest/network_architecture.html) as well as the [training procedure](https://dingo-gw.readthedocs.io/en/latest/training.html).\n",
    "\n",
    "Since we want to infer the posterior masses of GW150914, we fix the extrinsic prior values to the maximum likelihood values of the infered parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lqbIyHwavoUJ"
   },
   "outputs": [],
   "source": [
    "train_settings = \"\"\"\n",
    "data:\n",
    "  waveform_dataset_path: 01_training_data/waveform_dataset/waveform_dataset.hdf5  # Contains intrinsic waveforms\n",
    "  train_fraction: 0.95\n",
    "  window:  # Needed to calculate window factor for simulated data\n",
    "    type: tukey\n",
    "    f_s: 4096\n",
    "    T: 2.0\n",
    "    roll_off: 0.4\n",
    "  detectors:\n",
    "    - H1\n",
    "    - L1\n",
    "  extrinsic_prior:  # Sampled at train time\n",
    "    dec: bilby.core.prior.analytical.DeltaFunction(-1.2616009712219238)\n",
    "    ra: bilby.core.prior.analytical.DeltaFunction(1.4557750225067139)\n",
    "    geocent_time: bilby.core.prior.analytical.DeltaFunction(0.011423417367041111)\n",
    "    psi: bilby.core.prior.analytical.DeltaFunction(1.2124483585357666)\n",
    "    luminosity_distance: bilby.core.prior.analytical.DeltaFunction(488.2327880859375)\n",
    "  ref_time: 1126259462.391\n",
    "  inference_parameters:\n",
    "  - chirp_mass\n",
    "  - mass_ratio\n",
    "\n",
    "# Model architecture\n",
    "model:\n",
    "  type: nsf+embedding\n",
    "  # kwargs for neural spline flow\n",
    "  nsf_kwargs:\n",
    "    num_flow_steps: 5 # 30\n",
    "    base_transform_kwargs:\n",
    "      hidden_dim: 64 # 1024\n",
    "      num_transform_blocks: 5\n",
    "      activation: elu\n",
    "      dropout_probability: 0.0\n",
    "      batch_norm: True\n",
    "      num_bins: 8\n",
    "      base_transform_type: rq-coupling\n",
    "  # kwargs for embedding net\n",
    "  embedding_net_kwargs:\n",
    "    output_dim: 64 # 128\n",
    "    hidden_dims: [1024, 512, 256, 64]\n",
    "    activation: elu\n",
    "    dropout: 0.0\n",
    "    batch_norm: True\n",
    "    svd:\n",
    "      num_training_samples: 1000\n",
    "      num_validation_samples: 100\n",
    "      size: 50\n",
    "\n",
    "# The first stage (and only) stage of training.\n",
    "training:\n",
    "  stage_0:\n",
    "    epochs: 15\n",
    "    asd_dataset_path: 01_training_data/asd_dataset/asd_GW150914.hdf5\n",
    "    freeze_rb_layer: True\n",
    "    optimizer:\n",
    "      type: adam\n",
    "      lr: 0.0001\n",
    "    scheduler:\n",
    "      type: cosine\n",
    "      T_max: 15\n",
    "    batch_size: 64\n",
    "  # stage_1:\n",
    "  #   epochs: 5\n",
    "  #   asd_dataset_path: 01_training_data/asd_dataset/asd_GW150914.hdf5\n",
    "  #   freeze_rb_layer: False\n",
    "  #   optimizer:\n",
    "  #     type: adam\n",
    "  #     lr: 1.e-5\n",
    "  #   scheduler:\n",
    "  #     type: cosine\n",
    "  #     T_max: 5\n",
    "  #   batch_size: 64\n",
    "\n",
    "# Local settings for training that have no impact on the final trained network.\n",
    "local:\n",
    "  device: cuda  # [cpu, cuda] Set this to 'cuda' for training on a GPU.\n",
    "  num_workers: 2 # 6  # num_workers >0 does not work on Mac, see https://stackoverflow.com/questions/64772335/pytorch-w-parallelnative-cpp206\n",
    "  runtime_limits:\n",
    "    max_time_per_run: 36000\n",
    "    max_epochs_per_run: 30\n",
    "  checkpoint_epochs: 15\n",
    "\"\"\"\n",
    "train_settings = yaml.safe_load(train_settings)\n",
    "with open('02_training/train_settings.yaml', 'w') as outfile:\n",
    "    yaml.dump(train_settings, outfile, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mk5_2-Dcvotl"
   },
   "source": [
    "**Step 2: Start training run.**\n",
    "\n",
    "You can start the training by executing the following command. Note that a lot of helpful information is printed during training. This includes information about the SVD initialization (e.g. the truncation of the SVD basis and the resulting mismatch), how the new model is initialized, the number of fixed and learnable parameters of the model, the epoch number with training and validation loss, etc.\n",
    "\n",
    "Training for 20 epochs takes approximately 10 - 15 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "81J8e7dnlaGv",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "d0bbbbfa-f8b8-4eab-ac72-ea3efd212274"
   },
   "outputs": [],
   "source": [
    "!dingo_train \\\n",
    "--settings_file 02_training/train_settings.yaml \\\n",
    "--train_dir 02_training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-VF96La4kCoS"
   },
   "source": [
    "The loss values are stored in the `history.txt` file which contains as columns: epoch number, training loss, validation loss, and learning rate.\n",
    "\n",
    "We can load this file and plot the training and validation loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "id": "1sJr-YOLjcer",
    "outputId": "4e3a21b8-ce1d-40f7-c602-e71e12b94c40"
   },
   "outputs": [],
   "source": [
    "# Load history.txt\n",
    "filename = '02_training/history.txt'\n",
    "data = np.loadtxt(filename, delimiter=\"\\t\")\n",
    "\n",
    "# Plot loss values\n",
    "plt.plot(data[:,0], data[:,1], label=f\"training loss\")\n",
    "plt.plot(data[:,0], data[:,2], label=f\"validation loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1c1aHvTqlm7N"
   },
   "source": [
    "## 4. Injections\n",
    "\n",
    "By running [inference on an injection](https://dingo-gw.readthedocs.io/en/latest/example_injection.html) consistent with what the network was trained on, and then running the trained `dingo` model on it, we can get a first understanding of its performance.\n",
    "\n",
    "We use the **Sampler API** to prepare and run an injection:\n",
    "1. Load the model `dingo.core.models.posterior_model.PosteriorModel` into the `dingo.gw.inference.gw_samplers.GWSampler` class.\n",
    "2. Instantiate the `dingo.gw.injection.Injection` based on the settings the posterior model was trained on (accessible via `PosteriorModel.metadata`).\n",
    "3. Specify an ASD dataset, e.g. the fiducial ASD dataset the network was trained on.\n",
    "4. Sample from the prior and generate an injection.\n",
    "5. Insert the generated injection data into the sampler through `sampler.context`.\n",
    "6. Generate samples and convert the sampler instance into a `dingo.gw.result.Result`.\n",
    "7. Run importance sampling on the result object.\n",
    "8. Visualize the corner plot and save the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b-BTzAwWEiAY"
   },
   "outputs": [],
   "source": [
    "from dingo.core.models.posterior_model import PosteriorModel\n",
    "from dingo.gw.inference.gw_samplers import GWSampler\n",
    "from dingo.gw.injection import Injection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 964
    },
    "id": "5gTcp3G6E63q",
    "outputId": "3340ef72-3bbf-481a-c6ed-fe71b3ab1668"
   },
   "outputs": [],
   "source": [
    "model_path = \"02_training/model_latest.pt\"\n",
    "asd_path = \"01_training_data/asd_dataset/asd_GW150914.hdf5\"\n",
    "\n",
    "# Load the network into the GWSampler class\n",
    "pm = PosteriorModel(model_path, device=\"cuda\")\n",
    "sampler = GWSampler(model=pm)\n",
    "\n",
    "# Generate an injection consistent with the data the model was trained on.\n",
    "injection = Injection.from_posterior_model_metadata(pm.metadata)\n",
    "injection.asd = ASDDataset(asd_path, ifos=[\"H1\", \"L1\"])\n",
    "theta = injection.prior.sample()\n",
    "inj = injection.injection(theta)\n",
    "\n",
    "# Generate 10,000 samples from the DINGO model based on the generated injection data.\n",
    "sampler.context = inj\n",
    "sampler.run_sampler(10_000)\n",
    "result = sampler.to_result()\n",
    "\n",
    "# The following are only needed for importance-sampling the result.\n",
    "result.importance_sample(num_processes=8)\n",
    "\n",
    "# Make a corner plot and save the result.\n",
    "result.print_summary()\n",
    "kwargs = {\"legend_font_size\": 15, \"truth_color\": \"black\"}\n",
    "result.plot_corner(parameters=[\"chirp_mass\", \"mass_ratio\"],\n",
    "                   filename=\"03_inference/injection/corner.pdf\",\n",
    "                   truths=theta,\n",
    "                   **kwargs)\n",
    "result.to_file(\"03_inference/injection/result.hdf5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fl7Un8Sn-jvZ"
   },
   "source": [
    "The officially reported values are: chirp mass $\\mathcal{M}_c = 27.6^{+2.0}_{-2.0} M_{âŠ™}$, and mass ratio $q = 0.82^{+0.17}_{-0.20}$ (from [GWOSC](https://gwosc.org/events/GW150914/#P150914), [arXiV:1602.03837](https://arxiv.org/abs/1602.03837)).\n",
    "Since we trained the model in a simplified setting (small waveform dataset, small model, short training), we don't expect high sample efficiencies (> 1%)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gb3Yz2gRlfno"
   },
   "source": [
    "## 5. Real events with `dingo_pipe`\n",
    "\n",
    "Dingo includes a command-line tool [`dingo_pipe`](https://dingo-gw.readthedocs.io/en/latest/dingo_pipe.html) for automating inference tasks. This is based very closely on the [`bilby_pipe`](https://lscsoft.docs.ligo.org/bilby_pipe/master/user-interface.html) package, with suitable modifications.\n",
    "\n",
    "**Step 1: Prepare `.ini` file for `dingo_pipe`.**\n",
    "Similar to `bilby_pipe`, `dingo_pipe` requires an `.ini` file as an input that specifies the arguments of the subsequently performed steps.\n",
    "1. **Job submission arguments**: Specify cluster and system specific details here.\n",
    "2. **Sampler arguments**: Determine which model should be used to generate the samples.\n",
    "3. **Data generation arguments**: Define which event data should be downloaded.\n",
    "4. **Plotting arguments**: Specify which plots should be generated automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kmI3PWYtEyEK"
   },
   "outputs": [],
   "source": [
    "dingo_pipe_GW150914 = \"\"\"\n",
    "################################################################################\n",
    "##  Job submission arguments\n",
    "################################################################################\n",
    "\n",
    "local = True\n",
    "submit = False\n",
    "accounting = dingo\n",
    "request-cpus-importance-sampling = 2\n",
    "simple-submission = False\n",
    "\n",
    "################################################################################\n",
    "##  Sampler arguments\n",
    "################################################################################\n",
    "\n",
    "model = 02_training/model_latest.pt\n",
    "device = cuda\n",
    "num-samples = 5000\n",
    "batch-size = 5000\n",
    "recover-log-prob = true\n",
    "importance-sampling-settings = {}\n",
    "\n",
    "################################################################################\n",
    "## Data generation arguments\n",
    "################################################################################\n",
    "\n",
    "trigger-time = 1126259462.3885767 # GW150914 # condition network on maxlog geocent time 1126259462.4 - 0.011423417367041111\n",
    "label = GW150914\n",
    "outdir = 03_inference/outdir_GW150914\n",
    "channel-dict = {H1:GWOSC, L1:GWOSC}\n",
    "psd-length = 128\n",
    "# sampling-frequency = 2048.0\n",
    "# importance-sampling-updates = {'duration': 4.0}\n",
    "\n",
    "################################################################################\n",
    "## Plotting arguments\n",
    "################################################################################\n",
    "\n",
    "plot-corner = true\n",
    "plot-weights = true\n",
    "plot-log-probs = true\n",
    "\"\"\"\n",
    "with open('03_inference/GW150914.ini', 'w') as outfile:\n",
    "    outfile.write(dingo_pipe_GW150914)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ESQyNZ9UEydD"
   },
   "source": [
    "**Step 2: Run `dingo_pipe` for GW150914**\n",
    "\n",
    "If `local=True`, you can run all steps of `dingo_pipe` automatically by executing the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ky-pl9tili4L",
    "outputId": "123a284b-253d-4658-b263-2010b35d1be6"
   },
   "outputs": [],
   "source": [
    "!dingo_pipe 03_inference/GW150914.ini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZZy58i40C8Ki"
   },
   "source": [
    "This script produces several files in the folder `03_inference/outdir_GW150914`, where `results` contains the generated (importance) samples, and different plots. Similar to the injections, we don't expect good results from this simplified toy setting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4mk6pLoMNWLV"
   },
   "source": [
    "**Exercises**:\n",
    "1. Replace the delta function over the luminosity distance in the prior of the DINGO model by a Uniform distribution over $d_L \\in [100, 1000]$ Mpc.\n",
    "2. Train a marginalized model & perform inference.\n",
    "3. Train a model that infers the luminosity distance as well.\n",
    "\n",
    "You can use the folder `04_exercise` and its subfolders to save the settings files and models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_9nA80C7m_VI"
   },
   "source": [
    "# 6. Inference with a pre-trained DINGO model (from Zenodo)\n",
    "\n",
    "After illustrating all steps at a toy example, the rest of the tutorial will focus on running inference with a pretrained `dingo` model that can be downloaded from Zenodo. These production models rely on the algorithm [\"Group Equivariant Neural Posterior Estimation\" (GNPE)](https://dingo-gw.readthedocs.io/en/latest/gnpe.html) which includes (approximate) joint symmetries of data and parameters in the `dingo` model and simplifies the data subject to time shifts.\n",
    "\n",
    "Practically, this introduces only one change:\n",
    "Instead of one `dingo` model, we have to deal with two separate models:\n",
    "1. **Initialization model**: This model learns a proxy for the arrival times of the signal in each detector and acts as a starting point for the Gibbs sampler. Based on the output of this model, the strain data is transformed and simplified before serving as an input to the main model.\n",
    "2. **Main model**: This network is conditioned on the proxy variable and learns the full posterior distribution.\n",
    "\n",
    "\n",
    "**Step 1: Download trained DINGO model from Zenodo**\n",
    "\n",
    "We start by downloading the O1 GNPE networks from Zenodo (see [here](https://zenodo.org/records/12156303)). They were trained with `IMRPhenomXPHM`, and a luminosity distance prior of $d_L \\in [100 - 1000]$ Mpc.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wnbcPG7A_B6U"
   },
   "outputs": [],
   "source": [
    "!wget -q --directory-prefix=05_pretrained_model/init_train_dir https://zenodo.org/records/12156303/files/model_init.pt # init model\n",
    "!wget -q --directory-prefix=05_pretrained_model/main_train_dir https://zenodo.org/records/12156303/files/model.pt # main model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r39wvOn6rNBu"
   },
   "source": [
    "**Step 2: Create compatible .ini file**\n",
    "\n",
    "Since we are now working with a GNPE model, the `GW150914.ini` file has to be adapted. We need to specify the path to the initial as well as the main model and need to include the argument `num_gnpe_iterations` which determines the number of Gibbs sampling iterations.\n",
    "\n",
    "We set `recover-log-prob = False` and `importance-sample = False`, to speed up execution for the tutorial. If you change these values to `True`, the log probability values have to be recovered for importance sampling with GNPE which takes more time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "puWRrqSLq7v2"
   },
   "outputs": [],
   "source": [
    "dingo_pipe_GW150914 = \"\"\"\n",
    "################################################################################\n",
    "##  Job submission arguments\n",
    "################################################################################\n",
    "\n",
    "local = True\n",
    "submit = False\n",
    "accounting = dingo\n",
    "request-cpus-importance-sampling = 2\n",
    "simple-submission = False\n",
    "\n",
    "################################################################################\n",
    "##  Sampler arguments\n",
    "################################################################################\n",
    "\n",
    "model-init = 05_pretrained_model/init_train_dir/model_init.pt\n",
    "model = 05_pretrained_model/main_train_dir/model.pt\n",
    "device = cuda\n",
    "num-gnpe-iterations = 5 #30\n",
    "num-samples = 5000\n",
    "batch-size = 5000\n",
    "recover-log-prob = False\n",
    "prior-dict = {\n",
    "luminosity_distance = bilby.gw.prior.UniformComovingVolume(minimum=100, maximum=2000, name='luminosity_distance'),\n",
    "}\n",
    "importance-sample = False\n",
    "\n",
    "################################################################################\n",
    "## Data generation arguments\n",
    "################################################################################\n",
    "\n",
    "trigger-time = GW150914\n",
    "label = GW150914\n",
    "outdir = 05_pretrained_model/outdir_GW150914\n",
    "channel-dict = {H1:GWOSC, L1:GWOSC}\n",
    "psd-length = 128\n",
    "# sampling-frequency = 2048.0\n",
    "# importance-sampling-updates = {'duration': 4.0}\n",
    "\n",
    "################################################################################\n",
    "## Plotting arguments\n",
    "################################################################################\n",
    "\n",
    "plot-corner = true\n",
    "plot-weights = true\n",
    "plot-log-probs = true\n",
    "\"\"\"\n",
    "with open('05_pretrained_model/GW150914.ini', 'w') as outfile:\n",
    "    outfile.write(dingo_pipe_GW150914)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E0SIkaFEYdDq"
   },
   "source": [
    "**Step 3: Run `dingo_pipe` on pretrained DINGO model**\n",
    "\n",
    "Finally, we can run `dingo_pipe` with the specified `.ini` file to obtain the results for the complete posterior distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cPFEGcr_Xlpi",
    "outputId": "0f9a18c3-5ec3-4ebb-8c30-b52874f9f382"
   },
   "outputs": [],
   "source": [
    "!dingo_pipe 05_pretrained_model/GW150914.ini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "11QlQ1XeBHC7"
   },
   "source": [
    "The full 14-dimensional corner plot based on DINGO samples can be found in `05_pretrained_model/outdir_GW150914/results`.\n",
    "\n",
    "**Exercises:**\n",
    "1. Run an injection with the GNPE model. You can find some hints in the [GNPE](https://dingo-gw.readthedocs.io/en/latest/gnpe.html) and [injection](https://dingo-gw.readthedocs.io/en/latest/example_injection.html) docs.\n",
    "\n",
    "You will need to use the `GNPESampler` class that can be imported via\n",
    "`from dingo.core.samplers import GNPESampler`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rv9Fg1AeZPAh"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
